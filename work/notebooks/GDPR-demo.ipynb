{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29d58b6-a0d1-490b-b827-7b8783ce03c8",
   "metadata": {},
   "source": [
    "This notebook is a demonstration of how to handle GDPR export and delete requests in an Iceberg Lakehouse to make it GDPR-compliant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3aed1d-923a-4927-a29f-2c27e92c47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions, Row, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efae28a-499a-4ac2-a7e1-6c3d4a357c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(Path(sys.path[0]).resolve().parent.joinpath(\"modules\")))\n",
    "\n",
    "from data_faker import generate_users_dataframe, generate_events_dataframe, generate_gdpr_request_dataframe\n",
    "from stats import get_users_export_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab3adf-332e-43f2-8aad-c67526f2d059",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca0a207-e85c-40c6-aeca-34fa04269fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .setMaster(\"local[*]\")\n",
    "    .set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0,org.apache.iceberg:iceberg-aws-bundle:1.6.0\")\n",
    "    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog.type\", \"hive\")\n",
    "    .set(\"spark.sql.catalog.lakehouse\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.sql.catalog.lakehouse.type\", \"rest\")\n",
    "    .set(\"spark.sql.catalog.lakehouse.uri\", \"http://catalog:8181\")\n",
    "    .set(\"spark.sql.catalog.lakehouse.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .set(\"spark.sql.catalog.lakehouse.s3.endpoint\", \"http://minio:9000\")\n",
    "    .set(\"spark.sql.catalog.lakehouse.s3.path-style-access\", \"true\")\n",
    "    .set(\"spark.sql.catalog.lakehouse.s3.access-key-id\", os.environ[\"AWS_ACCESS_KEY_ID\"])\n",
    "    .set(\"spark.sql.catalog.lakehouse.s3.secret-access-key\", os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "    .set(\"spark.sql.catalog.lakehouse.client.region\", os.environ[\"AWS_REGION\"])\n",
    ")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9a738-9778-4897-9003-b42191d08c91",
   "metadata": {},
   "source": [
    "### Clean and create `demo` namespace in lakehouse catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c121e9a7-8970-4725-807b-162eb33f087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_db = \"lakehouse.demo\"\n",
    "if spark.catalog.databaseExists(demo_db):\n",
    "    for table in spark.catalog.listTables(dbName=demo_db):\n",
    "        spark.sql(f\"DROP TABLE {demo_db}.{table.name}\")\n",
    "    spark.sql(f\"DROP NAMESPACE {demo_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c246be4f-b1b8-47a0-b6c0-1069de5bf347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE NAMESPACE {demo_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943dfb9e-d4f0-4616-94be-e66b64fb25df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|demo     |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW NAMESPACES IN lakehouse\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4e52c-5ce3-4433-a294-c0502778bee0",
   "metadata": {},
   "source": [
    "### Generate Users, Events and GDPR requests random data and create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df67315b-c88c-46ce-a2aa-5e5281d86161",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table_name = f\"{demo_db}.users\"\n",
    "events_table_name = f\"{demo_db}.events\"\n",
    "gdpr_requests_table_name = f\"{demo_db}.gdpr_requests\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b79a3f1-0b7e-4415-b6b6-cb101a243fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100k users and add them to the users table\n",
    "users_df = generate_users_dataframe(spark, 100000)\n",
    "(\n",
    "    users_df\n",
    "    .writeTo(users_table_name)\n",
    "    .createOrReplace()\n",
    ")\n",
    "del users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a50253e9-8156-4623-8d48-3dc9ba7f0e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(users_table_name).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9405b32-7b01-4454-97c5-bb82c17694f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+--------------------------+------------+------------------------+\n",
      "|user_id                             |created_at                |name        |email                   |\n",
      "+------------------------------------+--------------------------+------------+------------------------+\n",
      "|ff0f450c-dc9c-49bc-9174-8db700cb7eb5|2024-08-26 18:48:44.607596|User38456842|user56165866@example.com|\n",
      "|bb89b19c-347b-4a73-a76b-2427478edf3e|2024-10-21 17:22:38.607856|User91917059|user51690866@example.com|\n",
      "+------------------------------------+--------------------------+------------+------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(users_table_name).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0c7d191-93fa-414b-922f-495529a00eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_table size: 1000000\n",
      "events_table size: 2000000\n",
      "events_table size: 3000000\n",
      "events_table size: 4000000\n",
      "events_table size: 5000000\n",
      "events_table size: 6000000\n",
      "events_table size: 7000000\n",
      "events_table size: 8000000\n",
      "events_table size: 9000000\n"
     ]
    }
   ],
   "source": [
    "# Generate 10M events and add them to the events table on 10 batchs of 1M to avoid memory issues\n",
    "events_df = generate_events_dataframe(spark, spark.table(users_table_name), 1000000)\n",
    "(\n",
    "    events_df\n",
    "    .writeTo(events_table_name)\n",
    "    # .partitionedBy(functions.days(functions.col(\"event_timestamp\")))\n",
    "    .createOrReplace()\n",
    ")\n",
    "for i in range(9):\n",
    "    print(f\"events_table size: {spark.table(events_table_name).count()}\")\n",
    "    events_df = generate_events_dataframe(spark, spark.table(users_table_name), 1000000)\n",
    "    events_df.writeTo(events_table_name).append()\n",
    "del events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fffcc64-4fb2-461d-848e-f5f3f7e4bf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(events_table_name).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "119918bb-2817-4acb-9435-2a460bc2ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------\n",
      " event_id        | 27115cac-e447-4cde-b235-48531f1e2a7b \n",
      " user_id         | 2a636d96-9c67-414c-b1f3-6f9198152771 \n",
      " event_type      | purchase                             \n",
      " event_timestamp | 2024-10-26 00:53:55.038357           \n",
      " page_url        | NULL                                 \n",
      " product_id      | product_61                           \n",
      " event_details   | Purchased product_61                 \n",
      "-RECORD 1-----------------------------------------------\n",
      " event_id        | a24e96ce-b296-4497-a815-73bb010df0f0 \n",
      " user_id         | 8374fb66-6a6b-4acd-81c1-a2e1ff5f26d3 \n",
      " event_type      | purchase                             \n",
      " event_timestamp | 2024-11-10 14:14:03.038363           \n",
      " page_url        | NULL                                 \n",
      " product_id      | product_92                           \n",
      " event_details   | Purchased product_92                 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(events_table_name).show(2, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "367a528f-531b-4d67-adc3-bc36e6e3f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 delete requests and 1000 export requests and add them to gdpr_requests table\n",
    "gdpr_requests = generate_gdpr_request_dataframe(spark, spark.table(users_table_name), 100, 1000)\n",
    "\n",
    "gdpr_requests.writeTo(gdpr_requests_table_name).createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b9992a-3db0-4102-8992-ea150b9481d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------+\n",
      "|user_id                             |request_type|\n",
      "+------------------------------------+------------+\n",
      "|cffc431f-9ff2-424f-b420-70e3113fea4e|delete      |\n",
      "|ea3d97b5-4567-406a-89de-3df72eb11411|delete      |\n",
      "|4f495cf7-e15e-4683-a322-b0c5b7d63181|delete      |\n",
      "|41a9f8bf-e782-4a6f-8805-921033a46836|delete      |\n",
      "|9bd5b479-a026-48f2-8a36-d7ddb6850d8e|delete      |\n",
      "|4e7d01f9-ae1e-4194-961d-3cdd64f71396|delete      |\n",
      "|606da03b-b96e-4746-a493-6942f7ae0232|delete      |\n",
      "|7869b5dc-88b0-4744-a1e0-2ac268ae47b5|delete      |\n",
      "|153bbdfb-d1f8-47ba-a938-633fa2805ae2|delete      |\n",
      "|7121fee3-bcb7-44d1-bbdd-169a0ea57f56|delete      |\n",
      "+------------------------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(gdpr_requests_table_name).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba7a3c-0399-4b3f-9b0c-78827c9c0e94",
   "metadata": {},
   "source": [
    "### Join events table with gdpr_requests table to get the expected rows to delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28b22190-4b23-402e-93b6-fcef6dac8507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9992"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM {events_table_name} events\n",
    "INNER JOIN {gdpr_requests_table_name} gdpr_requests\n",
    "ON gdpr_requests.request_type = 'delete'\n",
    "    AND events.user_id = gdpr_requests.user_id\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5489b47-439a-4b94-b20a-e3b093d723af",
   "metadata": {},
   "source": [
    "### Delete events generated by deleted users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90caf9c4-1d36-4b6e-9782-6cf7adb906a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "DELETE FROM {events_table_name} AS t\n",
    "WHERE EXISTS (\n",
    "    SELECT user_id\n",
    "    FROM {gdpr_requests_table_name}\n",
    "    WHERE request_type = 'delete'\n",
    "        AND t.user_id = user_id\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b076c2c9-a3cc-46ee-81de-8a0373dd4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_last_snapshot_id = spark.sql(f\"\"\"\n",
    "WITH events_table_history AS (\n",
    "    SELECT\n",
    "        snapshot_id,\n",
    "        ROW_NUMBER() OVER (ORDER BY made_current_at DESC) AS row_number\n",
    "    FROM {events_table_name}.history\n",
    ")\n",
    "SELECT snapshot_id\n",
    "FROM events_table_history\n",
    "WHERE row_number = 2;\n",
    "\"\"\").collect()[0][\"snapshot_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e1a8434-1afd-4c77-a854-e0fb956e39c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|previous_snapshot_id|current_snapshot_id|\n",
      "+--------------------+-------------------+\n",
      "| 2106750013844698407|1882728075265419825|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "   CALL lakehouse.system.rollback_to_snapshot('{events_table_name}', {before_last_snapshot_id})\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a7fdf9a-a446-4d25-b375-094891fc7575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {events_table_name} t\n",
    "USING (\n",
    "    SELECT *\n",
    "    FROM {gdpr_requests_table_name}\n",
    "    WHERE request_type = 'delete'\n",
    ") s\n",
    "ON t.user_id = s.user_id\n",
    "WHEN MATCHED THEN DELETE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0c0c339-deb8-4f66-87df-2aeffc227b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9990008"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(events_table_name).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c550b14-445b-468f-afae-87b429702014",
   "metadata": {},
   "source": [
    "### Re-join events table with gdpr_requests table to check if there is any match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba15c858-92ad-4049-a8eb-2bd847d6b099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM {events_table_name} events\n",
    "INNER JOIN {gdpr_requests_table_name} gdpr_requests\n",
    "ON gdpr_requests.request_type = 'delete'\n",
    "    AND events.user_id = gdpr_requests.user_id\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1569b-926f-4c4e-a65e-a9aeff41a364",
   "metadata": {},
   "source": [
    "### Re-join the old snapshot of events table with gdpr_requests table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6efadabe-1d6d-466c-98da-4068d258ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_last_snapshot_id = spark.sql(f\"\"\"\n",
    "WITH events_table_history AS (\n",
    "    SELECT\n",
    "        snapshot_id,\n",
    "        ROW_NUMBER() OVER (ORDER BY made_current_at DESC) AS row_number\n",
    "    FROM {events_table_name}.history\n",
    ")\n",
    "SELECT snapshot_id\n",
    "FROM events_table_history\n",
    "WHERE row_number = 2;\n",
    "\"\"\").collect()[0][\"snapshot_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e467945-2761-4f8f-b679-657516d4565d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9992"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT *\n",
    "    FROM {events_table_name}\n",
    "    VERSION AS OF {before_last_snapshot_id}\n",
    ") events\n",
    "INNER JOIN {gdpr_requests_table_name} gdpr_requests\n",
    "ON gdpr_requests.request_type = 'delete'\n",
    "    AND events.user_id = gdpr_requests.user_id\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb99312-985e-4125-beb4-d15fd23b8848",
   "metadata": {},
   "source": [
    "### Clean old snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20d2beea-a05e-44c4-ba9c-c32db881b05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " deleted_data_files_count            | 89  \n",
      " deleted_position_delete_files_count | 0   \n",
      " deleted_equality_delete_files_count | 0   \n",
      " deleted_manifest_files_count        | 21  \n",
      " deleted_manifest_lists_count        | 11  \n",
      " deleted_statistics_files_count      | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CALL lakehouse.system.expire_snapshots('{events_table_name}', TIMESTAMP '{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')}', 1)\n",
    "\"\"\").show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7ded1b8-fdbd-47ea-b155-942a88ddc9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM {events_table_name}.history\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151aa4b7-7d4c-49b6-b4da-49487276d692",
   "metadata": {},
   "source": [
    "### Process export requests\n",
    "\n",
    "When processing export requests, we typically want to group each user's data to write it to a single file or join it to other tables to retrieve the original values ​​when pseudonymization techniques are used on columns containing sensitive information.\n",
    "\n",
    "To simulate these scenarios, we write the result to a Parquet table partitioned by `user_id` to force Spark to shuffle the data using this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8506034d-9964-40b5-ae37-9920cbad69f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished in 132.024236561032s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.monotonic()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT events.*\n",
    "FROM {events_table_name} events\n",
    "INNER JOIN {gdpr_requests_table_name} gdpr_requests\n",
    "ON gdpr_requests.request_type = 'export'\n",
    "    AND events.user_id = gdpr_requests.user_id\n",
    "\"\"\").write.mode(\"overwrite\").partitionBy(\"user_id\").parquet(\"users_export_data1\")\n",
    "\n",
    "elapsed_time_without_optimization = time.monotonic() - start_time\n",
    "print(f\"finished in {elapsed_time_without_optimization}s\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e428e-594d-4d8b-9221-8aceb86d7d21",
   "metadata": {},
   "source": [
    "We now optimize the table using the `rewrite_data_files` stored procedure, sorting the data files by `user_id` in ascending order, and then rerun the export query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21d0beea-9b7f-4273-99fb-8cec062fb7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------\n",
      " rewritten_data_files_count | 40        \n",
      " added_data_files_count     | 1         \n",
      " rewritten_bytes_count      | 479267521 \n",
      " failed_data_files_count    | 0         \n",
      "\n",
      "Optimized the table in 22.89085567597067s\n"
     ]
    }
   ],
   "source": [
    "# Optimize the data files - sorting by user_id\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CALL lakehouse.system.rewrite_data_files(\n",
    "    table => '{events_table_name}',\n",
    "    strategy => 'sort',\n",
    "    sort_order => 'user_id ASC',\n",
    "    options => map('rewrite-all', 'true')\n",
    ")\n",
    "\"\"\").show(truncate=False, vertical=True)\n",
    "\n",
    "elapsed_time_for_optimization = time.monotonic() - start_time\n",
    "print(f\"Optimized the table in {elapsed_time_for_optimization}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f406965c-32c0-4274-96c1-f1cee98e3b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished in 4.196367917989846s\n"
     ]
    }
   ],
   "source": [
    "# rerun the export query on the optimized files\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT events.*\n",
    "FROM {events_table_name} events\n",
    "INNER JOIN {gdpr_requests_table_name} gdpr_requests\n",
    "ON gdpr_requests.request_type = 'export'\n",
    "    AND events.user_id = gdpr_requests.user_id\n",
    "\"\"\").write.mode(\"overwrite\").partitionBy(\"user_id\").parquet(\"users_export_data2\")\n",
    "\n",
    "elapsed_time_with_optimization = time.monotonic() - start_time\n",
    "print(f\"finished in {elapsed_time_with_optimization}s\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6279657-ab6f-44bf-b819-f35a438b0fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's 31.46X faster\n"
     ]
    }
   ],
   "source": [
    "print(f\"It's {(elapsed_time_without_optimization / elapsed_time_with_optimization):.2f}X faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "762d3c69-3193-450c-9963-9aa563d81436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two dataframes\n",
    "\n",
    "spark.read.parquet(\"users_export_data1\").createOrReplaceTempView(\"users_export_data1\")\n",
    "spark.read.parquet(\"users_export_data2\").createOrReplaceTempView(\"users_export_data2\")\n",
    "\n",
    "assert spark.table(\"users_export_data1\").count() == spark.table(\"users_export_data2\").count()\n",
    "\n",
    "assert spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM users_export_data1 v1 ANTI JOIN users_export_data2 v2\n",
    "    ON v1.user_id = v2.user_id AND v1.event_id = v2.event_id\n",
    "\"\"\").count() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83cdff68-858b-4f3a-9100-9b4d165420b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_files_by_user': 40,\n",
       " 'min_files_by_user': 29,\n",
       " 'avg_files_by_user': 36.753753753753756}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_users_export_stats(\"users_export_data1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cf97782-8124-4566-a424-b6dba2d9281b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_files_by_user': 1, 'min_files_by_user': 1, 'avg_files_by_user': 1.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_users_export_stats(\"users_export_data2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef625dba-dbe7-473d-8220-f351a657c479",
   "metadata": {},
   "source": [
    "As you can see, querying the optimized table is not only faster, but also produces significantly fewer files, which will help improve all of its downstream queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
